{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"llmio","text":""},{"location":"#this-documentation-is-a-work-in-progress","title":"This documentation is a work in progress","text":""},{"location":"#a-lightweight-python-library-for-llm-io","title":"\ud83c\udf88 A Lightweight Python Library for LLM I/O","text":"<p>Welcome to llmio! If you're looking for a simple, efficient way to build LLM-based agents, you've come to the right place.</p> <p>llmio is a lightweight Python library that leverages type annotations to make tool execution with OpenAI-compatible APIs effortless. Whether you're working with OpenAI, Azure OpenAI, Google Gemini, AWS Bedrock, or Huggingface TGI, llmio has you covered.</p>"},{"location":"#why-choose-llmio","title":"Why choose llmio?","text":"<ul> <li>Lightweight \ud83e\udeb6: Designed to integrate smoothly into your project without adding unnecessary bulk.</li> <li>Type Annotations \ud83c\udff7\ufe0f: Easily define tools with Python's type annotations and let llmio handle the rest.</li> <li>Broad API Compatibility \ud83c\udf0d: Seamlessly works with major APIs like OpenAI, Azure, Google Gemini, AWS, and Huggingface.</li> </ul>"},{"location":"#getting-started","title":"Getting Started \ud83d\ude80","text":"<p>Get started quickly with a simple installation:</p> <pre><code>pip install llmio\n</code></pre> <p>Set Up Your Agent: Start building with a few lines of code:</p> <pre><code>import asyncio\nfrom llmio import Agent, OpenAIClient\n\n\nagent = Agent(\n    instruction=\"You are a task manager.\",\n    client=OpenAIClient(api_key=\"your_openai_api_key\"),\n)\n\n# Add tools and interact with your agent...\n</code></pre>"},{"location":"#examples","title":"Examples","text":""},{"location":"#a-simple-calculator-example","title":"\ud83d\udcbb A simple calculator example","text":"<p>Let\u2019s walk through a basic example where we create a simple calculator using llmio. This calculator can add and multiply numbers, leveraging AI to handle the operations. It\u2019s a straightforward way to see how llmio can manage tasks while keeping the code clean and easy to follow.</p> <pre><code>import asyncio\nimport os\n\nfrom llmio import Agent, OpenAIClient\n\n\n# Define an agent that can add and multiply numbers using tools.\n# The agent will also print any messages it receives.\nagent = Agent(\n    # Define the agent's instructions.\n    instruction=\"\"\"\n        You are a calculating agent.\n        Always use tools to calculate things.\n        Never try to calculate things on your own.\n        \"\"\",\n    # Pass in an OpenAI client that will be used to interact with the model.\n    # Any API that implements the OpenAI interface can be used.\n    client=OpenAIClient(api_key=os.environ[\"OPENAI_TOKEN\"]),\n    model=\"gpt-4o-mini\",\n)\n\n\n# Define tools using the `@agent.tool` decorator.\n# Tools are automatically parsed by their type annotations\n# and added to the agent's capabilities.\n# The code itself is never seen by the LLM, only the function signature is exposed.\n# When the agent invokes a tool, the corresponding function is executed locally.\n@agent.tool\nasync def add(num1: float, num2: float) -&gt; float:\n    print(f\"** Executing add({num1}, {num2}) -&gt; {num1 + num2}\")\n    return num1 + num2\n\n\n# Tools can also be synchronous.\n@agent.tool\ndef multiply(num1: float, num2: float) -&gt; float:\n    print(f\"** Executing multiply({num1}, {num2}) -&gt; {num1 * num2}\")\n    return num1 * num2\n\n\n# Define a message handler using the `@agent.on_message` decorator.\n# The handler is optional. The messages will also be returned by the `speak` method.\n@agent.on_message\nasync def print_message(message: str):\n    print(f\"** Posting message: '{message}'\")\n\n\nasync def main():\n    # Run the agent with a message.\n    # The agent will return a response containing the messages it generated and the updated history.\n    response = await agent.speak(\"Hi! how much is 1 + 1?\")\n    # The agent is stateless and does not remember previous messages by itself.\n    # The history must be passed in to maintain context.\n    response = await agent.speak(\n        \"and how much is that times two?\", history=response.history\n    )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n# Output:\n# ** Executing add(1.0, 1.0) -&gt; 2.0\n# ** Posting message: '1 + 1 is 2.'\n# ** Executing multiply(2.0, 2.0) -&gt; 4.0\n# ** Posting message: 'That times two is 4.'\n</code></pre>"},{"location":"#more-examples","title":"More examples","text":"<p>For more examples, see <code>examples/</code>.</p> <p>For a notebook going throught how to create a simple AI task manager, see examples/notebooks/simple_task_manager.ipynb.</p>"},{"location":"#details","title":"Details \ud83d\udd0d","text":""},{"location":"#tools","title":"Tools","text":"<p>Under the hood, llmio uses Python's type annotations to automatically generate function schemas that are compatible with OpenAI tools. It also leverages Pydantic models to validate the input types of arguments passed by the language model, ensuring robust and error-free execution.</p> <pre><code>@agent.tool\nasync def add(num1: float, num2: float) -&gt; float:\n    \"\"\"\n    The docstring is used as the description of the tool.\n    \"\"\"\n    return num1 + num2\n\n\nprint(agent.summary())\n</code></pre> <p>Output: <pre><code>Tools:\n  - add\n    Schema:\n      {'description': 'The docstring is used as the description of the tool.',\n       'name': 'add',\n       'parameters': {'properties': {'num1': {'type': 'number'},\n                                     'num2': {'type': 'number'}},\n                      'required': ['num1', 'num2'],\n                      'type': 'object'},\n       'strict': False}\n</code></pre></p>"},{"location":"#parameter-descriptions","title":"Parameter descriptions","text":"<p>You can use pydantic.Field to describe parameters in detail. These descriptions will be included in the tool schema, guiding the language model to understand the tool's requirements better.</p> <pre><code>@agent.tool\nasync def book_flight(\n    destination: str = Field(..., description=\"The destination airport\"),\n    origin: str = Field(..., description=\"The origin airport\"),\n    date: datetime = Field(\n        ..., description=\"The date of the flight. ISO-format is expected.\"\n    ),\n) -&gt; str:\n    \"\"\"Books a flight\"\"\"\n    return f\"Booked flight from {origin} to {destination} on {date}\"\n</code></pre>"},{"location":"#optional-parameters","title":"Optional parameters","text":"<p>llmio supports optional parameters seamlessly.</p> <pre><code>@agent.tool\nasync def create_task(name: str = \"My task\", description: str | None = None) -&gt; str:\n    return \"Created task\"\n</code></pre>"},{"location":"#supported-parameter-types","title":"Supported parameter types","text":"<p>llmio supports the types that are supported by Pydantic. For more details, refer to Pydantic's documentation.</p>"},{"location":"#hooks","title":"Hooks","text":"<p>You can add hooks to receive callbacks with prompts and outputs. The names of the hooks are flexible as long as they are decorated appropriately.</p> <pre><code>@agent.on_message\nasync def on_message(message: str):\n    # on_message will be called with new messages from the model\n    pprint(prompt)\n\n@agent.inspect_prompt\nasync def inspect_prompt(prompt: list[llmio.Message]):\n    # inspect_prompt will be called with the prompt before it is sent to the model\n    pprint(prompt)\n\n\n@agent.inspect_output\nasync def inspect_output(output: llmio.Message):\n    # inspect_output will be called with the full model output\n    pprint(output)\n</code></pre>"},{"location":"#keeping-track-of-context","title":"Keeping track of context","text":"<p>Pass an object of any type to the agent to maintain context across interactions. This context is available to tools and hooks via the special <code>_context</code> argument but is not passed to the language model itself.</p> <pre><code>@dataclass\nclass User:\n    name: str\n\n\n@agent.tool\nasync def create_task(task_name: str, _context: User) -&gt; str:\n    print(f\"** Created task '{task_name}' for user '{_context.name}'\")\n    return \"Created task\"\n\n@agent.on_message\nasync def on_message(message: str, _context: User) -&gt; None:\n    print(f\"** Sending message to user '{_context.name}': {message}\")\n\n\nasync def main() -&gt; None:\n    _ = await agent.speak(\n        \"Create a task named 'Buy milk'\",\n        _context=User(name=\"Alice\"),\n    )\n</code></pre>"},{"location":"#dynamic-instructions","title":"Dynamic instructions","text":"<p><code>llmio</code> allows you to inject dynamic content into your instructions using variable hooks. These hooks act as placeholders, filling in values at runtime.</p> <p>When an instruction contains a placeholder that matches the name of a variable hook, <code>llmio</code> will automatically replace it with the corresponding value returned by the hook. If a placeholder does not have a matching variable hook, a <code>MissingVariable</code> error will be raised.</p> <pre><code>agent = Agent(\n    instruction=\"\"\"\n        You are a task manager for a user named {user_name}.\n        The current time is {current_time}.\n    \"\"\",\n    ...\n)\n\n@agent.variable\ndef user_name(_context: User) -&gt; str:\n    return _context.name\n\n@agent.variable\nasync def current_time() -&gt; datetime:\n    return datetime.now()\n\n# Example of formatted instruction:\n# \"You are a task manager for a user named Alice.\n#  The current time is 2024-08-25 10:17:04.606621.\"\n</code></pre>"},{"location":"#batched-execution","title":"Batched execution","text":"<p>Since the <code>Agent</code> class is stateless, you can safely execute multiple messages in parallel using <code>asyncio.gather</code>.</p> <pre><code>async def main() -&gt; None:\n    await asyncio.gather(\n        agent.speak(\"Create a task named 'Buy milk'\", history=[], _context=User(name=\"Alice\")),\n        agent.speak(\"Create a task named 'Buy bread'\", history=[], _context=User(name=\"Bob\")),\n    )\n</code></pre>"},{"location":"#a-simple-example-of-continuous-interaction","title":"A simple example of continuous interaction","text":"<pre><code>@agent.on_message\nasync def print_message(message: str):\n    print(message)\n\n\nasync def main() -&gt; None:\n    history = []\n    while True:\n        response = await agent.speak(input(\"&gt;&gt;\"), history=history)\n        history = response.history\n</code></pre> <p>Alternatively, use the messages returned by the agent:</p> <pre><code>async def main() -&gt; None:\n    history = []\n\n    while True:\n        response = await agent.speak(input(\"&gt;&gt;\"), history=history)\n        history = response.history\n        for message in response.messages:\n            print(message)\n</code></pre>"},{"location":"#handling-uninterpretable-tool-calls","title":"Handling uninterpretable tool calls","text":"<p><code>llmio</code> allows you to handle uninterpretable tool calls gracefully. By default, the agent will raise an exception if it encounters an unrecognized tool or invalid arguments. However, you can configure it to provide feedback to the model instead.</p> <pre><code># Raises an exception for unrecognized tools or invalid arguments\nagent = Agent(\n    client=OpenAIClient(api_key=os.environ[\"OPENAI_TOKEN\"]),\n    model=\"gpt-4o-mini\",\n    graceful_errors=False,  # This is the default\n)\n\n# Provides feedback to the model for unrecognized tools or invalid arguments\nagent = Agent(\n    client=OpenAIClient(api_key=os.environ[\"OPENAI_TOKEN\"]),\n    model=\"gpt-4o-mini\",\n    graceful_errors=True,\n)\n</code></pre>"},{"location":"#strict-tool-mode","title":"Strict tool mode","text":"<p>OpenAI supports a strict mode for tools, ensuring that only valid arguments are passed according to the function schema. Enable this by setting <code>strict=True</code> in the tool decorator.</p> <pre><code>@agent.tool(strict=True)\nasync def add_task(name: str, description: str | None = None) -&gt; str:\n    ...\n</code></pre>"},{"location":"#structured-output","title":"Structured output","text":"<p><code>llmio</code> can return structured output from the messages it generates, ideal for more advanced use cases. This feature is currently supported by OpenAI and Azure OpenAI.</p> <pre><code>import asyncio\nfrom pprint import pprint\nfrom typing import Literal\n\nimport pydantic\nimport os\n\nfrom llmio import StructuredAgent, OpenAIClient\n\n\nclass OutputFormat(pydantic.BaseModel):\n    answer: str\n    detected_sentiment: Literal[\"positive\", \"negative\", \"neutral\"]\n\n\nagent = StructuredAgent(\n    instruction=\"Answer the questions and detect the user sentiment.\",\n    client=OpenAIClient(api_key=os.environ[\"OPENAI_TOKEN\"]),\n    model=\"gpt-4o-mini\",\n    response_format=OutputFormat,\n)\n\n\n@agent.on_message\nasync def print_message(message: OutputFormat):\n    print(type(message))\n    pprint(message.dict())\n\n\nasync def main() -&gt; None:\n    _ = await agent.speak(\"I am happy!\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n# Output:\n# &lt;class '__main__.OutputFormat'&gt;\n# {'answer': \"That's great to hear! Happiness is a wonderful feeling.\",\n#  'detected_sentiment': 'positive'}\n</code></pre>"},{"location":"#get-involved","title":"Get involved \ud83c\udf89","text":"<p>Your feedback, ideas, and contributions are welcome! Feel free to open an issue, submit a pull request, or start a discussion to help make <code>llmio</code> even better.</p>"}]}